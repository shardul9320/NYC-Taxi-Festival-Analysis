{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fbb959-df65-4422-9bc8-50f6fd49141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully in local mode!\n",
      "<pyspark.sql.session.SparkSession object at 0x736b45747ad0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This is the fix!\n",
    "# \"local[*]\" tells Spark to use all available CPU cores in this one container\n",
    "# as its \"cluster\".\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDA_Final_Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# If this works, it will print your new Spark session object\n",
    "print(\"Spark Session created successfully in local mode!\")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfedf5b-f390-4441-a3af-cec799ced130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-29 19:33:54--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.164.82.112, 3.164.82.160, 3.164.82.40, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.164.82.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49961641 (48M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-01.parquet.1’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  47.65M  5.45MB/s    in 17s     \n",
      "\n",
      "2025-10-29 19:34:12 (2.73 MB/s) - ‘yellow_tripdata_2024-01.parquet.1’ saved [49961641/49961641]\n",
      "\n",
      "total 96M\n",
      "-rw-r--r-- 1 jovyan users  12K Oct 29 19:33 Untitled.ipynb\n",
      "drwsrwsr-x 2 jovyan users 4.0K Oct 20  2023 work\n",
      "-rw-r--r-- 1 jovyan users  48M Mar 21  2024 yellow_tripdata_2024-01.parquet\n",
      "-rw-r--r-- 1 jovyan users  48M Mar 21  2024 yellow_tripdata_2024-01.parquet.1\n"
     ]
    }
   ],
   "source": [
    "# Download the January 2024 yellow taxi data file\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "# Check if the file is there. You should see it in the output.\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aa5fcd-8620-4293-a827-445d42120b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Spark Session V2 Ready ---\n",
      "<pyspark.sql.session.SparkSession object at 0x736b45747ad0>\n",
      "--- 2. Data Loaded Successfully ---\n",
      "--- 3. Upgraded Feature Engineering... ---\n",
      "New features added:\n",
      "+--------------------+----+-----+-----------+----------+\n",
      "|tpep_pickup_datetime|hour|month|day_of_week|is_weekend|\n",
      "+--------------------+----+-----+-----------+----------+\n",
      "| 2024-01-01 00:57:55|   0|    1|          2|         0|\n",
      "| 2024-01-01 00:03:00|   0|    1|          2|         0|\n",
      "| 2024-01-01 00:17:06|   0|    1|          2|         0|\n",
      "| 2024-01-01 00:36:38|   0|    1|          2|         0|\n",
      "| 2024-01-01 00:46:51|   0|    1|          2|         0|\n",
      "+--------------------+----+-----+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- 4. Upgraded Aggregation... ---\n",
      "New aggregated data:\n",
      "+------------+-----+-----------+----+----------+----------+\n",
      "|PULocationID|month|day_of_week|hour|is_weekend|trip_count|\n",
      "+------------+-----+-----------+----+----------+----------+\n",
      "|          75|    1|          2|   2|         0|        42|\n",
      "|           4|    1|          2|   2|         0|        40|\n",
      "|         246|    1|          2|   5|         0|        23|\n",
      "|          70|    1|          2|   9|         0|       117|\n",
      "|         138|    1|          2|  10|         0|      1035|\n",
      "+------------+-----+-----------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- 5. Upgraded ML Prep... ---\n",
      "--- 6. Data Split for Training/Testing ---\n",
      "--- 7. Starting Model Training... (This will take a few minutes) ---\n",
      "--- Model V2 Trained Successfully! ---\n",
      "--- 8. Making V2 Predictions... ---\n",
      "+--------------------+----------+------------------+\n",
      "|            features|trip_count|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|[1.0,1.0,1.0,8.0,...|         1|26.095738015462093|\n",
      "|[1.0,1.0,1.0,12.0...|         4|29.936851806612946|\n",
      "|[1.0,1.0,1.0,14.0...|         6| 34.35718755716563|\n",
      "|[1.0,1.0,1.0,19.0...|         3| 34.35718755716563|\n",
      "|[1.0,1.0,2.0,9.0,...|         3| 32.09182519000393|\n",
      "|[1.0,1.0,2.0,15.0...|         7|35.376269277871344|\n",
      "|[1.0,1.0,3.0,6.0,...|         1| 18.32235334226422|\n",
      "|[1.0,1.0,3.0,13.0...|         1|34.675052565750555|\n",
      "|[1.0,1.0,4.0,8.0,...|         3| 27.23806566136981|\n",
      "|[1.0,1.0,4.0,9.0,...|         1| 27.89552329620946|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "---  FINAL PROJECT V2 COMPLETE  ---\n",
      "OLD RMSE (V1): 1393.87\n",
      "NEW RMSE (V2): 240.61461620860186\n",
      "This means, on average, our NEW model's prediction is off by about 240 trips.\n"
     ]
    }
   ],
   "source": [
    "# --- Import all libraries (with new functions) ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, col, dayofweek, month, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# --- 1. Start Spark Session ---\n",
    "# Use .getOrCreate() to get the existing session or make a new one\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDA_Final_Project_V2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"--- 1. Spark Session V2 Ready ---\")\n",
    "print(spark)\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "# (Assumes 'yellow_tripdata_2024-01.parquet' is in your folder)\n",
    "filepath = \"yellow_tripdata_2024-01.parquet\"\n",
    "df = spark.read.parquet(filepath)\n",
    "print(\"--- 2. Data Loaded Successfully ---\")\n",
    "\n",
    "# --- 3. UPGRADED Feature Engineering ---\n",
    "print(\"--- 3. Upgraded Feature Engineering... ---\")\n",
    "# Clean the data\n",
    "df = df.dropna(subset=['tpep_pickup_datetime', 'PULocationID'])\n",
    "\n",
    "# Original feature\n",
    "df = df.withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# --- NEW FEATURES ---\n",
    "# Get the month (1-12)\n",
    "df = df.withColumn(\"month\", month(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Get the day of the week (1=Sunday, 2=Monday, ..., 7=Saturday)\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Create an 'is_weekend' feature (1 for weekend, 0 for weekday)\n",
    "df = df.withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0))\n",
    "\n",
    "print(\"New features added:\")\n",
    "df.select(\"tpep_pickup_datetime\", \"hour\", \"month\", \"day_of_week\", \"is_weekend\").show(5)\n",
    "\n",
    "# --- 4. UPGRADED Aggregation ---\n",
    "print(\"--- 4. Upgraded Aggregation... ---\")\n",
    "# We now group by ALL our features\n",
    "demand_df = df.groupBy(\"PULocationID\", \"month\", \"day_of_week\", \"hour\", \"is_weekend\").count()\n",
    "demand_df = demand_df.withColumnRenamed(\"count\", \"trip_count\")\n",
    "print(\"New aggregated data:\")\n",
    "demand_df.show(5)\n",
    "\n",
    "# --- 5. UPGRADED ML Preparation ---\n",
    "print(\"--- 5. Upgraded ML Prep... ---\")\n",
    "# This is our new, smarter feature list\n",
    "feature_cols = [\"PULocationID\", \"month\", \"day_of_week\", \"hour\", \"is_weekend\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "features_df = assembler.transform(demand_df)\n",
    "\n",
    "(training_data, test_data) = features_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"--- 6. Data Split for Training/Testing ---\")\n",
    "\n",
    "# --- 7. Train the V2 Model ---\n",
    "print(\"--- 7. Starting Model Training... (This will take a few minutes) ---\")\n",
    "rf = RandomForestRegressor(labelCol=\"trip_count\", featuresCol=\"features\")\n",
    "model_v2 = rf.fit(training_data)\n",
    "print(\"--- Model V2 Trained Successfully! ---\")\n",
    "\n",
    "# --- 8. Make & Evaluate V2 Predictions ---\n",
    "print(\"--- 8. Making V2 Predictions... ---\")\n",
    "predictions = model_v2.transform(test_data)\n",
    "predictions.select(\"features\", \"trip_count\", \"prediction\").show(10)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_v2 = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"---  FINAL PROJECT V2 COMPLETE  ---\")\n",
    "print(f\"OLD RMSE (V1): 1393.87\")\n",
    "print(f\"NEW RMSE (V2): {rmse_v2}\")\n",
    "print(f\"This means, on average, our NEW model's prediction is off by about {int(rmse_v2)} trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd455cb2-8c27-4a23-8905-f77c6dc15987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9. Saving final predictions to a CSV file... ---\n",
      "root\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- is_weekend: integer (nullable = false)\n",
      " |-- trip_count: long (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "---  File Saved!  ---\n",
      "A new folder named 'heatmap_predictions' has been created in your Jupyter 'work' directory.\n",
      "Inside, you will find a .csv file you can download and open in Tableau or Power BI.\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Save Final Results for Visualization ---\n",
    "print(\"--- 9. Saving final predictions to a CSV file... ---\")\n",
    "\n",
    "# Let's see all our columns\n",
    "predictions.printSchema()\n",
    "\n",
    "# We need the location, hour, day, and the prediction.\n",
    "# Let's save the predictions for a specific day to make it easy to visualize.\n",
    "# (e.g., Tuesday, January 16th, 2024 = day_of_week 3)\n",
    "heatmap_df = predictions.filter( (col(\"month\") == 1) & (col(\"day_of_week\") == 3) ) \\\n",
    "                        .select(\"PULocationID\", \"hour\", \"prediction\")\n",
    "\n",
    "# The 'predictions' DataFrame is big. Let's make it a small, single CSV.\n",
    "# .coalesce(1) forces Spark to save it as one file.\n",
    "# .repartition(1) is another way to do this.\n",
    "heatmap_df.repartition(1).write.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .save(\"heatmap_predictions\")\n",
    "\n",
    "print(\"---  File Saved!  ---\")\n",
    "print(\"A new folder named 'heatmap_predictions' has been created in your Jupyter 'work' directory.\")\n",
    "print(\"Inside, you will find a .csv file you can download and open in Tableau or Power BI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8897d629-b7e4-4d24-b8d4-3fcfe752c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Making a prediction for this single input: ---\n",
      "+------------+-----+-----------+----+----------+\n",
      "|PULocationID|month|day_of_week|hour|is_weekend|\n",
      "+------------+-----+-----------+----+----------+\n",
      "|         142|    1|          3|  17|         0|\n",
      "+------------+-----+-----------+----+----------+\n",
      "\n",
      "--- 2. Prediction Generated: ---\n",
      "+--------------------+-----------------+\n",
      "|            features|       prediction|\n",
      "+--------------------+-----------------+\n",
      "|[142.0,1.0,3.0,17...|388.2318295717943|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "---  FINAL PREDICTION  ---\n",
      "For Location ID 142 at hour 17,\n",
      "the model predicts a demand of: 388 trips.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Import the 'datetime' library ---\n",
    "# This lets us get the *current* time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --- STEP 2: Define your inputs ---\n",
    "# --- (You or your professor can change these!) ---\n",
    "test_location = 142  # (e.g., 142 = Grand Central)\n",
    "test_hour = 17       # (e.g., 5 PM)\n",
    "test_month = 1       # (January)\n",
    "test_day_of_week = 3 # (1=Sun, 2=Mon, 3=Tues)\n",
    "test_is_weekend = 0  # (0=No)\n",
    "\n",
    "# --- You can also use the CURRENT time! ---\n",
    "# now = datetime.now()\n",
    "# test_location = 142\n",
    "# test_hour = now.hour\n",
    "# test_month = now.month\n",
    "# test_day_of_week = (now.weekday() + 2) % 7 + 1 # (Converts Mon=0 to Sun=1)\n",
    "# test_is_weekend = 1 if test_day_of_week in (1, 7) else 0\n",
    "\n",
    "\n",
    "# --- STEP 3: Create a 1-row DataFrame with these inputs ---\n",
    "# We make a list of one \"row\"\n",
    "my_data = [(test_location, test_month, test_day_of_week, test_hour, test_is_weekend)]\n",
    "\n",
    "# We define the column names to match our model\n",
    "my_schema = [\"PULocationID\", \"month\", \"day_of_week\", \"hour\", \"is_weekend\"]\n",
    "\n",
    "# Create the 1-row Spark DataFrame\n",
    "test_df = spark.createDataFrame(my_data, schema=my_schema)\n",
    "\n",
    "print(\"--- 1. Making a prediction for this single input: ---\")\n",
    "test_df.show()\n",
    "\n",
    "# --- STEP 4: Use the V2 Assembler and Model ---\n",
    "# (This assumes your V2 assembler is named 'assembler')\n",
    "final_test_df = assembler.transform(test_df)\n",
    "prediction = model_v2.transform(final_test_df)\n",
    "\n",
    "print(\"--- 2. Prediction Generated: ---\")\n",
    "prediction.select(\"features\", \"prediction\").show()\n",
    "\n",
    "# --- STEP 5: Get the final number and print it! ---\n",
    "final_number = prediction.collect()[0][\"prediction\"]\n",
    "\n",
    "print(\"---  FINAL PREDICTION  ---\")\n",
    "print(f\"For Location ID {test_location} at hour {test_hour},\")\n",
    "print(f\"the model predicts a demand of: {int(final_number)} trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bd988-dbd9-4ea7-b274-d3f522fb7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---  FINAL PREDICTION  ---\")\n",
    "print(f\"For Location ID {test_location} at hour {test_hour},\")\n",
    "print(f\"the model predicts a demand of: {int(final_number)} trips.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
